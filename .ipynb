{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e1JYLMCaLl9M"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import logging\n",
    "import torchvision.datasets as dset\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I4bvnN_1ROUX"
   },
   "outputs": [],
   "source": [
    "# Modified AlexNet architecture from Krizhevsky et al. in 2012 \n",
    "# http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf\n",
    "class MNIST_target_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNIST_target_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3)\n",
    "\n",
    "        self.fc1 = nn.Linear(64*4*4, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.logits = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.logits(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zhFF8W1XROgz"
   },
   "outputs": [],
   "source": [
    "# Core AdvGAN model from Xiao et al. in 2019 - https://arxiv.org/pdf/1801.02610.pdf\n",
    "\n",
    "# Implements discriminator model which takes in the generated adversarial sample and outputs the probability that it is real.\n",
    "# This gives the generator feedback via backprop and results in more realistic adversarial samples\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_nc):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(image_nc, 8, kernel_size=4, stride=2, padding=0, bias=True)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=0, bias=True)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0, bias=True)\n",
    "        self.conv4 = nn.Conv2d(32, 1, 1)\n",
    "        # MNIST: 1*28*28\n",
    "        model = [\n",
    "            nn.Conv2d(image_nc, 8, kernel_size=4, stride=2, padding=0, bias=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 8*13*13\n",
    "            nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # 16*5*5\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "            # 32*1*1\n",
    "        ]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.model(x).squeeze()\n",
    "        return output\n",
    "\n",
    "# Implements generator model which takes in a random noise vector and creates an image resembling the data set\n",
    "# Takes feedback from the discriminator in order to generate more realistic samples\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self,\n",
    "                 gen_input_nc,\n",
    "                 image_nc,\n",
    "                 ):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        encoder_lis = [\n",
    "            # MNIST:1*28*28\n",
    "            nn.Conv2d(gen_input_nc, 8, kernel_size=3, stride=1, padding=0, bias=True),\n",
    "            nn.InstanceNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            # 8*26*26\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=0, bias=True),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            # 16*12*12\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, bias=True),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # 32*5*5\n",
    "        ]\n",
    "\n",
    "        bottle_neck_lis = [ResnetBlock(32),\n",
    "                       ResnetBlock(32),\n",
    "                       ResnetBlock(32),\n",
    "                       ResnetBlock(32),]\n",
    "\n",
    "        decoder_lis = [\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=0, bias=False),\n",
    "            nn.InstanceNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            # state size. 16 x 11 x 11\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=0, bias=False),\n",
    "            nn.InstanceNorm2d(8),\n",
    "            nn.ReLU(),\n",
    "            # state size. 8 x 23 x 23\n",
    "            nn.ConvTranspose2d(8, image_nc, kernel_size=6, stride=1, padding=0, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. image_nc x 28 x 28\n",
    "        ]\n",
    "\n",
    "\n",
    "        self.encoder = nn.Sequential(*encoder_lis)\n",
    "        self.bottle_neck = nn.Sequential(*bottle_neck_lis)\n",
    "        self.decoder = nn.Sequential(*decoder_lis)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.bottle_neck(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# ResnetBlock implementation from https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/models/networks.py\n",
    "# In a network with residual blocks, each layer feeds into the next layer as well as a few layers beyond\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, padding_type='reflect', norm_layer=nn.BatchNorm2d, use_dropout=False, use_bias=False):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n",
    "\n",
    "    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n",
    "        conv_block = []\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
    "                       norm_layer(dim),\n",
    "                       nn.ReLU(True)]\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        p = 0\n",
    "        if padding_type == 'reflect':\n",
    "            conv_block += [nn.ReflectionPad2d(1)]\n",
    "        elif padding_type == 'replicate':\n",
    "            conv_block += [nn.ReplicationPad2d(1)]\n",
    "        elif padding_type == 'zero':\n",
    "            p = 1\n",
    "        else:\n",
    "            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n",
    "\n",
    "        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n",
    "                       norm_layer(dim)]\n",
    "\n",
    "        return nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.conv_block(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "Cvv-QQ5RbrOH",
    "outputId": "ca302da8-b2e3-4a8c-8fdd-dc4edcf8a88a"
   },
   "outputs": [],
   "source": [
    "# Prepares training and testing data\n",
    "root = './data'\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "#trans = transforms.ToTensor()\n",
    "trans = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_set = dset.MNIST(root=root, train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_set = dset.MNIST(root=root, train=False, transform=transforms.ToTensor(), download=True)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=128,\n",
    "                 shuffle=True, num_workers = 0)\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                shuffle=False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SEgcvOA8ROkY",
    "outputId": "ad79c476-1f5e-4795-e011-d7a76508b7f2"
   },
   "outputs": [],
   "source": [
    "# training the target model\n",
    "target_model = MNIST_target_net()\n",
    "target_model.train()\n",
    "optim = torch.optim.SGD(target_model.parameters(), lr=1e-3)\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    loss_epoch = 0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "      train_imgs, train_labels = data\n",
    "      train_imgs, train_labels = train_imgs, train_labels\n",
    "      print(train_labels)\n",
    "      target_model.eval()\n",
    "      logits_model = target_model(train_imgs)\n",
    "      target_model.train()\n",
    "      print(torch.argmax(logits_model,1))\n",
    "      loss_model = F.cross_entropy(logits_model, train_labels)\n",
    "      print(loss_model)\n",
    "      loss_epoch += loss_model\n",
    "      loss_model.backward()\n",
    "      optim.step()\n",
    "      optim.zero_grad()\n",
    "      print('batch done')\n",
    "    print('loss in epoch %d: %f' % (epoch, loss_epoch.item()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xyj6fRog7sy5",
    "outputId": "4f775e23-d168-4c25-c37f-9fbdebeee75c"
   },
   "outputs": [],
   "source": [
    "# loads weights of trained target model\n",
    "pretrained_model = 'MNIST_target_model.pth'\n",
    "target_model = MNIST_target_net()\n",
    "target_model.load_state_dict(torch.load(pretrained_model))\n",
    "target_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "upaIQeNDROnO",
    "outputId": "fa665177-00dd-4124-eb04-1ddcf8a8a0f5"
   },
   "outputs": [],
   "source": [
    "# tests trained model on testing data as a baseline\n",
    "target_model.train(False)\n",
    "target_model.to('cpu')\n",
    "num_correct = 0\n",
    "for i, data in enumerate(test_dataloader, 0):\n",
    "        test_img, test_label = data\n",
    "        pred_lab = torch.argmax(target_model(test_img), 1)\n",
    "        num_correct += torch.sum(pred_lab==test_label,0)\n",
    "        print(test_label)\n",
    "        print(pred_lab)\n",
    "\n",
    "print('accuracy in testing set: %f\\n'%(num_correct.item()/len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2Ql1OYuROya"
   },
   "outputs": [],
   "source": [
    "# Model mutation testing approach from Wang et al. in 2019 - https://arxiv.org/pdf/1812.05793.pdf\n",
    "\n",
    "def mut_test(model, test_loader, verbose=False,device=torch.cuda.get_device_name(0)):\n",
    "      model = model.to(device)\n",
    "      model.eval()\n",
    "      test_loss = 0\n",
    "      correct = 0\n",
    "      batch_size = 128\n",
    "      data_size = len(test_loader.dataset)\n",
    "      time_count = []\n",
    "      with torch.no_grad():\n",
    "          for data, target in test_loader:\n",
    "              data, target = data.to(device), target.to(device)\n",
    "              output = model(data)\n",
    "              test_loss += F.nll_loss(output, target, size_average=False).item()  # sum up batch loss\n",
    "              pred = output.max(1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "              if verbose:\n",
    "                  sys.stdout.write('\\r progress:{:.2f}%'.format((1.*batch_size*progress*100)/data_size))\n",
    "\n",
    "      test_loss /= len(test_loader.dataset)\n",
    "      acc = 1. * correct / len(test_loader.dataset)\n",
    "\n",
    "      if verbose:\n",
    "          print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.1f}%) time:{:.6f}\\n'.format(\n",
    "              test_loss, correct, len(test_loader.dataset),\n",
    "              100 * acc, np.average(time_count)))\n",
    "      return acc\n",
    "\n",
    "# defines mutation operators\n",
    "# NAI - neuron activation inverse\n",
    "# GF - gaussian fuzzing\n",
    "# WS - weight shuffling\n",
    "# NS - neuron switch\n",
    "\n",
    "class OpType(object):\n",
    "    NAI = 'NAI'\n",
    "    GF  = 'GF'\n",
    "    WS = 'WS'\n",
    "    NS = 'NS'\n",
    "\n",
    "\n",
    "class MutationOperator(object):\n",
    "    def __init__(self, ration, model, acc_tolerant=0.90, verbose=True,test=True,test_data_loader=None,device='cpu'):\n",
    "        '''\n",
    "        :param ration:\n",
    "        :param model:\n",
    "        :param acc_tolerant:\n",
    "        :param verbose: print the mutated detail or not. like the number of weights to be mutated with layer\n",
    "        :param test:\n",
    "        '''\n",
    "        self.ration = ration\n",
    "        self.original_model = model.to(device)\n",
    "        self.verbose = verbose\n",
    "        self.test_data_loader = test_data_loader\n",
    "        self.device = device\n",
    "\n",
    "        if test:\n",
    "            premier_acc = mut_test(self.original_model, self.test_data_loader,device=self.device)\n",
    "            logging.info('orginal model acc={0}'.format(premier_acc))\n",
    "            self.acc_threshold = round(premier_acc * acc_tolerant, 2)\n",
    "            logging.info('acc_threshold:{}%'.format(100 * self.acc_threshold))\n",
    "            \n",
    "    # weight level operator which fuzzes the weights using a gaussian distribution\n",
    "    \n",
    "    def gaussian_fuzzing(self, std=None):\n",
    "        mutation_model = copy.deepcopy(self.original_model)\n",
    "        num_weights = 0\n",
    "        num_layers = 0  \n",
    "        std_layers = [] \n",
    "        for param in mutation_model.parameters():\n",
    "            num_weights += (param.data.view(-1)).size()[0]\n",
    "            num_layers += 1\n",
    "            std_layers.append(param.data.std().item())\n",
    "\n",
    "        indices = np.random.choice(num_weights, int(num_weights * self.ration), replace=False)\n",
    "        weights_count = 0\n",
    "        for idx_layer, param in enumerate(mutation_model.parameters()):\n",
    "            shape = param.data.size()\n",
    "            num_weights_layer = (param.data.view(-1)).size()[0]\n",
    "            mutated_indices = set(indices) & set(\n",
    "                np.arange(weights_count, weights_count + num_weights_layer))\n",
    "\n",
    "            if mutated_indices:\n",
    "                mutated_indices = np.array(list(mutated_indices))\n",
    "                mutated_indices = mutated_indices - weights_count\n",
    "\n",
    "                current_weights = param.data.cpu().view(-1).numpy()\n",
    "                \n",
    "                avg_weights = np.mean(current_weights)\n",
    "                current_std = std if std else std_layers[idx_layer]\n",
    "                mutated_weights = np.random.normal(avg_weights, current_std, mutated_indices.size)\n",
    "\n",
    "                current_weights[mutated_indices] = mutated_weights\n",
    "                new_weights = torch.Tensor(current_weights).reshape(shape)\n",
    "                param.data = new_weights.to(self.device)\n",
    "\n",
    "        return mutation_model\n",
    "\n",
    "    # shuffles the weights of randomly selecting neuron slices in a layer\n",
    "    \n",
    "    def ws(self):\n",
    "        unique_neurons = 0\n",
    "        mutation_model = copy.deepcopy(self.original_model)\n",
    "        for param in mutation_model.parameters():\n",
    "            shape = param.size()\n",
    "            dim = len(shape)\n",
    "            if dim > 1:\n",
    "                unique_neurons += shape[0]\n",
    "\n",
    "        indices = np.random.choice(unique_neurons, int(unique_neurons * self.ration), replace=False)\n",
    "        neurons_count = 0\n",
    "        for idx_layer, param in enumerate(mutation_model.parameters()):\n",
    "            shape = param.size()\n",
    "            dim = len(shape)\n",
    "            if dim > 1:\n",
    "                unique_neurons_layer = shape[0]\n",
    "                mutated_neurons = set(indices) & set(np.arange(neurons_count, neurons_count + unique_neurons_layer))\n",
    "                if mutated_neurons:\n",
    "                    mutated_neurons = np.array(list(mutated_neurons)) - neurons_count\n",
    "                    for neuron in mutated_neurons:\n",
    "                        ori_shape = param.data[neuron].size()\n",
    "                        old_data = param.data[neuron].view(-1).cpu().numpy()\n",
    "                        shuffle_idx = np.arange(len(old_data))\n",
    "                        np.random.shuffle(shuffle_idx)\n",
    "                        new_data = old_data[shuffle_idx]\n",
    "                        new_data = torch.Tensor(new_data).reshape(ori_shape)\n",
    "                        param.data[neuron] = new_data.to(self.device)\n",
    "                neurons_count += unique_neurons_layer\n",
    "\n",
    "        return mutation_model\n",
    "\n",
    "    # switches two random neurons in a layer\n",
    "    \n",
    "    def ns(self, skip=10):\n",
    "        unique_neurons = 0\n",
    "        mutation_model = copy.deepcopy(self.original_model)\n",
    "        for idx_layer, param in enumerate(mutation_model.parameters()):\n",
    "            shape = param.size()\n",
    "            dim = len(shape)\n",
    "            unique_neurons_layer = shape[0]\n",
    "            if dim > 1 and unique_neurons_layer >= skip:\n",
    "                import math\n",
    "                temp = unique_neurons_layer * self.ration\n",
    "                num_mutated = math.floor(temp) if temp > 2. else math.ceil(temp)\n",
    "                mutated_neurons = np.random.choice(unique_neurons_layer,\n",
    "                                                   int(num_mutated), replace=False)\n",
    "                switch = copy.copy(mutated_neurons)\n",
    "                np.random.shuffle(switch)\n",
    "                param.data[mutated_neurons] = param.data[switch]\n",
    "        return mutation_model\n",
    "\n",
    "    # inverts the activation status of a neuron by changing the sign of its output value\n",
    "    \n",
    "    def nai(self):\n",
    "        unique_neurons = 0\n",
    "        mutation_model = copy.deepcopy(self.original_model)\n",
    "        for param in mutation_model.parameters():\n",
    "            shape = param.size()\n",
    "            dim = len(shape)\n",
    "            if dim > 1:\n",
    "                unique_neurons += shape[0]\n",
    "        indices = np.random.choice(unique_neurons, int(unique_neurons * self.ration), replace=False)\n",
    "        neurons_count = 0\n",
    "        last_mutated_neurons = []\n",
    "        for idx_layer, param in enumerate(mutation_model.parameters()):\n",
    "            shape = param.size()\n",
    "            dim = len(shape)\n",
    "            if dim > 1:\n",
    "                unique_neurons_layer = shape[0]\n",
    "                mutated_neurons = set(indices) & set(np.arange(neurons_count, neurons_count + unique_neurons_layer))\n",
    "                if mutated_neurons:\n",
    "                    mutated_neurons = np.array(list(mutated_neurons)) - neurons_count\n",
    "                    param.data[mutated_neurons] = -1 * param.data[mutated_neurons]\n",
    "                    last_mutated_neurons = mutated_neurons\n",
    "                neurons_count += unique_neurons_layer\n",
    "            else:\n",
    "                param.data[last_mutated_neurons] = -1 * param.data[last_mutated_neurons]\n",
    "                last_mutated_neurons = []\n",
    "\n",
    "        return mutation_model\n",
    "\n",
    "    # chooses activation function\n",
    "    \n",
    "    def afr(self, act_type):\n",
    "        '''\n",
    "        :param act_type: the type of activation func\n",
    "        :return:\n",
    "        '''\n",
    "        model = copy.deepcopy(self.original_model)\n",
    "        ActFun = nn.ReLU if act_type == 'relu' else nn.ELU\n",
    "\n",
    "        num_actlayers = 0\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, ActFun):\n",
    "                num_actlayers += 1\n",
    "\n",
    "        if num_actlayers == 0:\n",
    "            raise Exception('No [{}] layer found'.format(ActFun))\n",
    "\n",
    "        temp = num_actlayers * self.ration\n",
    "        num_remove = 1 if temp < 1 else math.floor(temp)\n",
    "        num_remove = int(num_remove)\n",
    "        idces_remove = np.random.choice(num_actlayers, num_remove, replace=False)\n",
    "        print('>>>>>>>idces_remove:{}'.format(idces_remove))\n",
    "        idx = 0\n",
    "        for name, module in model.named_children():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                if idx in idces_remove:\n",
    "                    model.__delattr__(name)\n",
    "                idx += 1\n",
    "            else:\n",
    "                for grand_name, child in module.named_children():\n",
    "                    if isinstance(child, nn.ReLU):\n",
    "                        if idx in idces_remove:\n",
    "                            module.__delattr__(grand_name)\n",
    "                        idx += 1\n",
    "        print(model)\n",
    "        return model\n",
    "      \n",
    "    # determines whether the mutated model is qualified or not\n",
    "\n",
    "    def __is_qualified(self, mutated_model):\n",
    "        acc = mut_test(mutated_model, self.test_data_loader,device=self.device)\n",
    "        if round(acc,2) < self.acc_threshold:\n",
    "            logging.info('Warning: bad accurate {0},reproduce mutated model'.format(acc))\n",
    "            return False\n",
    "        logging.info('Mutated model: accurate {0}'.format(acc))\n",
    "        return True\n",
    "\n",
    "    # assures that the accuracy of the mutant model satifies the specified thresholds and returns the qualified model\n",
    "    \n",
    "    def filter(self, f, **kwargs):\n",
    "        qualified = False\n",
    "        while not qualified:\n",
    "            mutated_model = f(**kwargs)\n",
    "            qualified = self.__is_qualified(mutated_model)\n",
    "        return mutated_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rcc4WSq9M7BJ"
   },
   "outputs": [],
   "source": [
    "# specify training parameters\n",
    "image_nc=1\n",
    "epochs = 60\n",
    "batch_size = 128\n",
    "BOX_MIN = 0\n",
    "BOX_MAX = 1\n",
    "model_num_labels = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39bB3-BVPTIW"
   },
   "outputs": [],
   "source": [
    "# custom weight initialization used in previous AdvGAN implementation to assure fairness\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same training procedure from AdvGAB implementation in https://github.com/mathcbc/advGAN_pytorch/blob/master/advGAN.py\n",
    "\n",
    "class Mutating_GAN_Attack:\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 device,\n",
    "                 model_num_labels,\n",
    "                 image_nc,\n",
    "                 box_min,\n",
    "                 box_max,\n",
    "                 train_dataloader):\n",
    "        output_nc = image_nc\n",
    "        self.model_num_labels = model_num_labels\n",
    "        self.device = device\n",
    "        self.input_nc = image_nc\n",
    "        self.output_nc = output_nc\n",
    "        self.box_min = box_min\n",
    "        self.box_max = box_max\n",
    "        self.gen_input_nc = image_nc\n",
    "        self.netG = Generator(self.gen_input_nc, image_nc).to(device)\n",
    "        self.netG.to('cuda')\n",
    "        self.netDisc = Discriminator(image_nc).to(device)\n",
    "        self.netDisc.to('cuda')\n",
    "        self.classifier = model\n",
    "        self.classifier.eval()\n",
    "        self.dataloader = train_dataloader\n",
    "\n",
    "        self.netG.apply(weights_init)\n",
    "        self.netDisc.apply(weights_init)\n",
    "        self.operator = operator = MutationOperator(ration=0.2, model=self.classifier, test=False, test_data_loader = self.dataloader)\n",
    "\n",
    "        self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
    "                                            lr=0.001)\n",
    "        self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
    "                                            lr=0.001)\n",
    "        self.trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "        \n",
    "    def train_batch(self, x, labels, mutate):\n",
    "        # optimize D\n",
    "        for i in range(1):\n",
    "            perturbation = self.netG(x)\n",
    "\n",
    "            # clipping trick used in https://github.com/mathcbc/advGAN_pytorch/blob/master/advGAN.py\n",
    "            adv_images = torch.clamp(perturbation, -0.3, 0.3) + x\n",
    "            adv_images = torch.clamp(adv_images, self.box_min, self.box_max)\n",
    "            adv_images_norm = torch.sub(adv_images, 0.1307)\n",
    "            adv_images_norm = torch.div(adv_images_norm, 0.3081)\n",
    "            self.optimizer_D.zero_grad()\n",
    "            pred_real = self.netDisc(x)\n",
    "            loss_D_real = F.mse_loss(pred_real, torch.ones_like(pred_real, device=self.device))\n",
    "            loss_D_real.backward()\n",
    "\n",
    "            pred_fake = self.netDisc(adv_images.detach())\n",
    "            loss_D_fake = F.mse_loss(pred_fake, torch.zeros_like(pred_fake, device=self.device))\n",
    "            loss_D_fake.backward()\n",
    "            loss_D_GAN = loss_D_fake + loss_D_real\n",
    "            self.optimizer_D.step()\n",
    "\n",
    "\n",
    "        for i in range(1):\n",
    "            self.optimizer_G.zero_grad()\n",
    "            pred_fake = self.netDisc(adv_images)\n",
    "            loss_G_fake = F.mse_loss(pred_fake, torch.ones_like(pred_fake, device=self.device))\n",
    "            loss_G_fake.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "            C = 0.1\n",
    "            loss_perturb = torch.mean(torch.norm(perturbation.view(perturbation.shape[0], -1), 2, dim=1))\n",
    "\n",
    "            self.classifier.to('cuda')\n",
    "            logits_model = self.classifier(adv_images_norm)\n",
    "            probs_model = F.softmax(logits_model, dim=1)\n",
    "            onehot_labels = torch.eye(self.model_num_labels, device=self.device)[labels]\n",
    "\n",
    "            real = torch.sum(onehot_labels * probs_model, dim=1)\n",
    "            other, _ = torch.max((1 - onehot_labels) * probs_model - onehot_labels * 10000, dim=1)\n",
    "            zeros = torch.zeros_like(other)\n",
    "            loss_adv = torch.max(real - other, zeros)\n",
    "            loss_adv = torch.sum(loss_adv)\n",
    "            #custom mutant loss\n",
    "            if mutate:\n",
    "                with torch.no_grad():\n",
    "                    labels = self.classifier(adv_images)\n",
    "                mutate_model = self.operator.ws()  \n",
    "                mutate_model.to('cuda')\n",
    "                with torch.no_grad():\n",
    "                    mutant_labels = mutate_model(adv_images)\n",
    "                mutate_loss = F.mse_loss(labels, mutant_labels)\n",
    "            else:\n",
    "                mutate_loss = 0\n",
    "            #loss_adv = -F.mse_loss(logits_model, onehot_labels)\n",
    "            #loss_adv = - F.cross_entropy(logits_model, labels)\n",
    "            adv_lambda = 10\n",
    "            pert_lambda = 1\n",
    "            mutate_lambda = 5\n",
    "            loss_G = adv_lambda * loss_adv + pert_lambda * loss_perturb + mutate_lambda * mutate_loss\n",
    "            loss_G.backward()\n",
    "            self.optimizer_G.step()\n",
    "        return loss_D_GAN.item(), loss_G_fake.item(), loss_perturb.item(), loss_adv.item(), mutate_loss\n",
    "\n",
    "    def train(self, epochs):\n",
    "        for epoch in range(1, epochs+1):\n",
    "\n",
    "            if epoch == 50:\n",
    "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
    "                                                    lr=0.0001)\n",
    "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
    "                                                    lr=0.0001)\n",
    "            if epoch == 80:\n",
    "                self.optimizer_G = torch.optim.Adam(self.netG.parameters(),\n",
    "                                                    lr=0.00001)\n",
    "                self.optimizer_D = torch.optim.Adam(self.netDisc.parameters(),\n",
    "                                                    lr=0.00001)\n",
    "            loss_D_sum = 0\n",
    "            loss_G_fake_sum = 0\n",
    "            loss_perturb_sum = 0\n",
    "            loss_adv_sum = 0\n",
    "            loss_mutate_sum = 0\n",
    "            for i, data in enumerate(self.dataloader, start=0):\n",
    "                images, labels = data\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                if epoch > 60:\n",
    "                    loss_D_batch, loss_G_fake_batch, loss_perturb_batch, loss_adv_batch, mutate_loss_batch = \\\n",
    "                        self.train_batch(images, labels, True)\n",
    "                else:\n",
    "                    loss_D_batch, loss_G_fake_batch, loss_perturb_batch, loss_adv_batch, mutate_loss_batch = \\\n",
    "                        self.train_batch(images, labels, False)\n",
    "                loss_D_sum += loss_D_batch\n",
    "                loss_G_fake_sum += loss_G_fake_batch\n",
    "                loss_perturb_sum += loss_perturb_batch\n",
    "                loss_adv_sum += loss_adv_batch\n",
    "                loss_mutate_sum += mutate_loss_batch\n",
    "\n",
    "            # print statistics\n",
    "            num_batch = len(self.dataloader)\n",
    "            print(\"epoch %d:\\nloss_D: %.3f, loss_G_fake: %.3f,\\\n",
    "             \\nloss_perturb: %.3f, loss_adv: %.3f, \\n, loss_mutate: %.3f, \\n\" %\n",
    "                  (epoch, loss_D_sum/num_batch, loss_G_fake_sum/num_batch,\n",
    "                   loss_perturb_sum/num_batch, loss_adv_sum/num_batch, loss_mutate_sum/num_batch))\n",
    "\n",
    "            # save generator\n",
    "            if epoch%2==0:\n",
    "                netG_file_name = 'netG_WS_epoch_' + str(epoch) + '.pth'\n",
    "                torch.save(self.netG.state_dict(), netG_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "colab_type": "code",
    "id": "B8Fv-tMzy_ty",
    "outputId": "6ac28ec3-3718-49c6-bdd6-e35908fb8584"
   },
   "outputs": [],
   "source": [
    "# trains MGAN\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "                 dataset=train_set,\n",
    "                 batch_size=32,\n",
    "                 shuffle=False, num_workers = 0)\n",
    "target_model = target_model.to('cuda')\n",
    "device = torch.device(\"cuda\")\n",
    "MGAN = Mutating_GAN_Attack(target_model,\n",
    "                          device,\n",
    "                          model_num_labels,\n",
    "                          image_nc,\n",
    "                          BOX_MIN,\n",
    "                          BOX_MAX,\n",
    "                          train_dataloader)\n",
    "MGAN.train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bqMJd6mrzJe4",
    "outputId": "366e8929-d75e-4fbb-ef87-5ae876d8ecb2"
   },
   "outputs": [],
   "source": [
    "use_cuda=True\n",
    "image_nc=1\n",
    "batch_size = 128\n",
    "\n",
    "gen_input_nc = image_nc\n",
    "\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
    "\n",
    "# load the generator of adversarial examples\n",
    "pretrained_generator_path = 'netG_epoch_62.pth'\n",
    "pretrained_G = Generator(gen_input_nc, image_nc)\n",
    "pretrained_G.load_state_dict(torch.load(pretrained_generator_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uRlD-DmBIqBS"
   },
   "outputs": [],
   "source": [
    "# tests the accuracy of MGAN\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "                dataset=test_set,\n",
    "                shuffle=False, num_workers = 0)\n",
    "num_correct = 0\n",
    "for i, data in enumerate(test_dataloader, 0):\n",
    "    test_img, test_label = data\n",
    "    test_img, test_label = test_img.to(device), test_label.to(device)\n",
    "    pretrained_G.to('cuda')\n",
    "    perturbation = pretrained_G(test_img)\n",
    "    adv_images = perturbation + test_img\n",
    "    perturbation = torch.clamp(perturbation, -0.3, 0.3)\n",
    "    adv_images = torch.clamp(adv_images, 0,1)\n",
    "    operator = MutationOperator(ration=0.001, model=target_model, test=False, test_data_loader = test_dataloader)\n",
    "    mutate_model = operator.nai()  \n",
    "    mutate_model.to('cuda')\n",
    "    target_model.to('cuda')\n",
    "    mutant_label = torch.argmax(mutate_model(test_img),1)\n",
    "    pred_lab = torch.argmax(target_model(test_img),1)\n",
    "    num_correct += torch.sum(mutant_label==pred_lab,0)\n",
    "    adv_images=torch.Tensor.cpu(adv_images).detach().numpy()[-1,-1,:,:]\n",
    "    if i % 1000 == 0:\n",
    "        print('done')\n",
    "        #fig = plt.figure()\n",
    "        #ax = plt.subplot(111)\n",
    "        #ax.imshow(adv_images, cmap = 'gray')\n",
    "        #plt.title('predicted: %d, actual: %d' %(pred_lab, test_label))\n",
    "        #fig.savefig('adv_img %d' %(i))\n",
    "print('num_correct: ', num_correct.item())\n",
    "print('accuracy of adv imgs in testing set: %f\\n'%(num_correct.item()/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate label change rate of adversarial samples as a measure of sensitivity\n",
    "\n",
    "num_incorrect = 0\n",
    "for i, data in enumerate(test_dataloader, 0):\n",
    "    test_img, test_label = data\n",
    "    perturbation = pretrained_G(test_img)\n",
    "    perturbation = torch.clamp(perturbation, -0.3, 0.3)\n",
    "    adv_img = perturbation + test_img\n",
    "    adv_img = torch.clamp(adv_img, 0, 1)\n",
    "    pred_mutant = torch.argmax(mutate_model(adv_img),1)\n",
    "    pred_lab = torch.argmax(target_model(adv_img),1)\n",
    "    num_incorrect += torch.sum(pred_lab!=pred_mutant,0)\n",
    "print('Label change rate of adversarial samples: %f\\n'%(num_correct.item()/len(test_dataloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
